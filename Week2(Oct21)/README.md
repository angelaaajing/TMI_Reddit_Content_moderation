# Week2(Oct21) 
# Transformer Architecture

Welcome to the Week 2 deliverable! During last week's meeting, we reviewed the past project and saw they adopted simple recurrent networks (RNN, LSTM, GRU) instead of Transformer-based models like BERT or GPT. This year, we are using Transformer-based models (via Vertex AI), which will significantly boost embedding accuracy.

So, what is a Transformer-based model? It's a relatively recent architecture, introduced in 2017, that replaces traditional models like RNN with a more powerful mechanism called self-attention. Self-attention enables the model to understand relationships between tokens with two components involved: (1) Encoder—for converting input text into embeddings, and (2) Decoder—for using embeddings from the encoder to generate or classify output.

Our tool, Vertex AI, uses this transformer-based architecture, with BERT being an encoder model and GPT being a decoder model. This week, we will focus on understanding how Transformer-based models use the Encoder-Decoder Architecture through the Google Cloud Skills Boost course.

Find all the resources here: https://www.cloudskillsboost.google/paths/183/course_templates/538
There is no need to finish the course, just take whatever is helpful to you.

